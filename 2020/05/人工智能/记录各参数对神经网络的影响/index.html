<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/recyclable.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/recyclable32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/recyclable16.png">
  <link rel="mask-icon" href="/images/recyclable.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-material.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"liaoooyx.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="神经网络中的常见问题梯度爆炸 在深层神经网络或循环神经网络中，误差的梯度可在更新中累积相乘。如果网络层之间的梯度值大于 1.0，那么重复相乘会导致梯度呈指数级增长，梯度变的非常大，然后导致网络权重的大幅更新，并因此使网络变得不稳定。 信号：  模型不稳定，导致更新过程中的损失出现显著变化；  训练过程中，在极端情况下，权重的值变得非常大，以至于溢出，导致模型损失变成 NaN等等。   解决梯度爆">
<meta property="og:type" content="article">
<meta property="og:title" content="摘录各参数对神经网络的影响">
<meta property="og:url" content="https://liaoooyx.com/2020/05/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E8%AE%B0%E5%BD%95%E5%90%84%E5%8F%82%E6%95%B0%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BD%B1%E5%93%8D/index.html">
<meta property="og:site_name" content="米达的博客">
<meta property="og:description" content="神经网络中的常见问题梯度爆炸 在深层神经网络或循环神经网络中，误差的梯度可在更新中累积相乘。如果网络层之间的梯度值大于 1.0，那么重复相乘会导致梯度呈指数级增长，梯度变的非常大，然后导致网络权重的大幅更新，并因此使网络变得不稳定。 信号：  模型不稳定，导致更新过程中的损失出现显著变化；  训练过程中，在极端情况下，权重的值变得非常大，以至于溢出，导致模型损失变成 NaN等等。   解决梯度爆">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=0.25%5E%7B10%7D%5Capprox0.000000954">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=W_1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Calpha+">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf3bno6qd9j31hs0ju794.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf43xiyod1g30h80dc4n1.gif">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf43xmkc6bg30h80dc1ca.gif">
<meta property="article:published_time" content="2020-05-23T23:00:00.000Z">
<meta property="article:modified_time" content="2020-05-24T21:58:26.797Z">
<meta property="article:author" content="liaoooyx">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.zhihu.com/equation?tex=0.25%5E%7B10%7D%5Capprox0.000000954">

<link rel="canonical" href="https://liaoooyx.com/2020/05/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E8%AE%B0%E5%BD%95%E5%90%84%E5%8F%82%E6%95%B0%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BD%B1%E5%93%8D/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>摘录各参数对神经网络的影响 | 米达的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="米达的博客" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">米达的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">好好学习</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    

  <a href="https://github.com/liaoooyx" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://liaoooyx.com/2020/05/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E8%AE%B0%E5%BD%95%E5%90%84%E5%8F%82%E6%95%B0%E5%AF%B9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BD%B1%E5%93%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="liaoooyx">
      <meta itemprop="description" content="坐标英国利兹">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="米达的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          摘录各参数对神经网络的影响
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-05-24 00:00:00 / 修改时间：22:58:26" itemprop="dateCreated datePublished" datetime="2020-05-24T00:00:00+01:00">2020-05-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">笔记</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>10 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <a id="more"></a>
<h3 id="神经网络中的常见问题"><a href="#神经网络中的常见问题" class="headerlink" title="神经网络中的常见问题"></a>神经网络中的常见问题</h3><h4 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h4><blockquote>
<p>在深层神经网络或循环神经网络中，误差的梯度可在更新中累积相乘。如果网络层之间的梯度值大于 1.0，那么重复相乘会导致梯度呈指数级增长，梯度变的非常大，然后导致网络权重的大幅更新，并因此使网络变得不稳定。</p>
<p>信号：</p>
<ul>
<li><p>模型不稳定，导致更新过程中的损失出现显著变化；</p>
</li>
<li><p>训练过程中，在极端情况下，权重的值变得非常大，以至于溢出，导致模型损失变成 NaN等等。</p>
</li>
</ul>
<p>解决梯度爆炸的其他方法：</p>
<ol>
<li>减少学习率（个人理解梯度爆炸是模型训练发散的一种情况）；</li>
<li>使用ReLU函数，使得梯度稳定；</li>
<li>使用正则化，即检查网络中权重的大小，对较大的权重进行惩罚，限制了梯度爆炸造成的权重变得很大的情况。</li>
</ol>
<p>——<a href="https://zhuanlan.zhihu.com/p/37217242" target="_blank" rel="noopener">常见损失函数小结</a></p>
</blockquote>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><blockquote>
<ol>
<li><strong>神经网络为什么需要激活函数：</strong>首先数据的分布绝大多数是非线性的，而一般神经网络的计算是线性的，引入激活函数，是在神经网络中引入非线性，强化网络的学习能力。所以激活函数的最大特点就是非线性。</li>
<li><strong>不同的激活函数，根据其特点，应用也不同。</strong>Sigmoid和tanh的特点是将输出限制在(0,1)和(-1,1)之间，说明Sigmoid和tanh适合做概率值的处理，例如LSTM中的各种门；而ReLU就不行，因为ReLU无最大值限制，可能会出现很大值。同样，根据ReLU的特征，Relu适合用于深层网络的训练，而Sigmoid和tanh则不行，因为它们会出现梯度消失。</li>
</ol>
<p>——<a href="https://zhuanlan.zhihu.com/p/73214810" target="_blank" rel="noopener">激活函数总结</a></p>
</blockquote>
<h4 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h4><blockquote>
<p>sigmoid函数作为激活函数本身就存在梯度消失的问题（见缺点3）</p>
<p>——<a href="https://zhuanlan.zhihu.com/p/37217242" target="_blank" rel="noopener">常见损失函数小结</a></p>
<p>优缺点：</p>
<ul>
<li>优点：平滑、易于求导。</li>
<li>缺点：<ol>
<li>激活函数计算量大（在正向传播和反向传播中都包含幂运算和除法）；</li>
<li>反向传播求误差梯度时，求导涉及除法；</li>
<li>Sigmoid导数取值范围是[0, 0.25]，由于神经网络反向传播时的“链式反应”，很容易就会出现梯度消失的情况。例如对于一个10层的网络， 根据<img src="https://www.zhihu.com/equation?tex=0.25^{10}\approx0.000000954" alt="[公式]">，第10层的误差相对第一层卷积的参数<img src="https://www.zhihu.com/equation?tex=W_1" alt="[公式]">的梯度将是一个非常小的值，这就是所谓的“梯度消失”。</li>
<li>Sigmoid的输出不是0均值（即zero-centered）；这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入，随着网络的加深，会改变数据的原始分布。</li>
</ol>
</li>
</ul>
<p>——<a href="https://zhuanlan.zhihu.com/p/73214810" target="_blank" rel="noopener">激活函数总结</a></p>
</blockquote>
<h4 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h4><blockquote>
<p>相比Sigmoid函数</p>
<ol>
<li>tanh的输出范围时(-1, 1)，解决了Sigmoid函数的不是zero-centered输出问题；</li>
<li>幂运算的问题仍然存在；</li>
<li>tanh导数范围在(0, 1)之间，相比sigmoid的(0, 0.25)，梯度消失（gradient vanishing）问题会得到缓解，但仍然还会存在。</li>
</ol>
<p>——<a href="https://zhuanlan.zhihu.com/p/73214810" target="_blank" rel="noopener">激活函数总结</a></p>
</blockquote>
<h4 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h4><p><strong>特点</strong></p>
<blockquote>
<p>ReLU的有效导数是常数1，解决了深层网络中出现的梯度消失问题，也就使得深层网络可训练。<strong>同时ReLU又是非线性函数，所谓非线性，就是一阶导数不为常数；对ReLU求导，在输入值分别为正和为负的情况下，导数是不同的，即ReLU的导数不是常数，所以ReLU是非线性的（只是不同于Sigmoid和tanh，relu的非线性不是光滑的）。</strong></p>
<p>——<a href="https://zhuanlan.zhihu.com/p/73214810" target="_blank" rel="noopener">激活函数总结</a></p>
<p>relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。</p>
<p><strong>relu</strong>的主要贡献在于：</p>
<ul>
<li>解决了梯度消失、爆炸的问题</li>
<li>计算方便，计算速度快</li>
<li>加速了网络的训练</li>
</ul>
<p>同时也存在一些<strong>缺点</strong>：</p>
<ul>
<li>由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）</li>
<li>输出不是以0为中心的</li>
</ul>
<p>——<a href="https://zhuanlan.zhihu.com/p/76772734" target="_blank" rel="noopener">从反向传播推导到梯度消失and爆炸的原因及解决方案（从DNN到RNN，内附详细反向传播公式推导）</a></p>
</blockquote>
<p><strong>ReLU函数相对于tanh和sigmoid函数好在哪里：</strong></p>
<blockquote>
<p>采用sigmoid等函数，算激活函数是（指数运算），计算量大；反向传播求误差梯度时，求导涉及除法，计算量相对大。而采用Relu激活函数，整个过程的计算量节省很多。</p>
<p>对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0），这种情况会造成信息丢失，梯度消失在网络层数多的时候尤其明显，从而无法完成深层网络的训练。</p>
<p>ReLU会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。</p>
<p>——<a href="https://zhuanlan.zhihu.com/p/37217242" target="_blank" rel="noopener">常见损失函数小结</a></p>
<p>相比Sigmoid和tanh，ReLU摒弃了复杂的计算，提高了运算速度。</p>
<p>解决了梯度消失问题，收敛速度快于Sigmoid和tanh函数，但要<strong>防范ReLU的梯度爆炸</strong></p>
<p>容易得到更好的模型，但也要<strong>防止训练中出现模型【Dead】情况</strong>。</p>
<p>——<a href="https://zhuanlan.zhihu.com/p/73214810" target="_blank" rel="noopener">激活函数总结</a></p>
</blockquote>
<p><strong>在使用relu的网络中，是否还存在梯度消失的问题？</strong></p>
<blockquote>
<p>梯度衰减的原因包括：激活函数导数，此外，还有多个权重连乘也会影响。梯度消失只是表面说法，按照这样理解，底层使用非常大的学习率，或者人工添加梯度噪音，原则上也能回避，有不少论文这样试了，然而目前来看，有用，但没太大的用处。深层原因训练不好的本质难题可能不是衰减或者消失（残差网络论文也提到这一点），是啥目前数理派也搞不清楚，所以写了论文也顺势这样说开了。不然，贸贸然将开山鼻祖的观点否定了，是需要极大勇气和大量的实验，以及中二精神的。</p>
<p>—— <a href="https://www.zhihu.com/question/49230360/answer/114914080" target="_blank" rel="noopener">知乎问题</a>，回答 <a href="https://www.zhihu.com/people/lykquitphy" target="_blank" rel="noopener">纳米酱</a></p>
</blockquote>
<h4 id="Leaky-ReLU-PReLU（Parametric-Relu）-RReLU（Random-ReLU）"><a href="#Leaky-ReLU-PReLU（Parametric-Relu）-RReLU（Random-ReLU）" class="headerlink" title="Leaky ReLU, PReLU（Parametric Relu）, RReLU（Random ReLU）"></a>Leaky ReLU, PReLU（Parametric Relu）, RReLU（Random ReLU）</h4><blockquote>
<p>为了防止模型的【Dead】情况，后人将x&lt;0部分并没有直接置为0，而是给了一个很小的负数梯度值</p>
<p><strong>Leaky ReLU</strong>中的<img src="https://www.zhihu.com/equation?tex=\alpha" alt="[公式]">为常数，一般设置 0.01。这个函数通常比 Relu 激活函数效果要好，但是效果不是很稳定，所以在实际中 Leaky ReLu 使用的并不多。</p>
<p><strong>PRelu（参数化修正线性单元）</strong> 中的<img src="https://www.zhihu.com/equation?tex=\alpha+" alt="[公式]">作为一个可学习的参数，会在训练的过程中进行更新。</p>
<p><strong>RReLU（随机纠正线性单元）</strong>也是Leaky ReLU的一个变体。在RReLU中，负值的斜率在训练中是随机的，在之后的测试中就变成了固定的了。RReLU的亮点在于，在训练环节中，aji是从一个均匀的分布U(I,u)中随机抽取的数值。</p>
<p>——<a href="https://zhuanlan.zhihu.com/p/73214810" target="_blank" rel="noopener">激活函数总结</a></p>
</blockquote>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><blockquote>
<p>损失函数是网络学习的指挥棒，它引导着网络学习的方向——能让损失函数变小的参数就是好参数。所以，损失函数的选择和设计要能表达你希望模型具有的性质与倾向。——<a href="https://me.csdn.net/firelx" target="_blank" rel="noopener">shine-lee</a>（<a href="https://blog.csdn.net/blogshinelee/article/details/103518097" target="_blank" rel="noopener">原文</a>）</p>
</blockquote>
<h4 id="交叉熵损失"><a href="#交叉熵损失" class="headerlink" title="交叉熵损失"></a>交叉熵损失</h4><p>（Pytorch）CrossEntropyLoss本身已经包含了softmax，在神经网络中使用CrossEntropyLoss作为损失函数时，不需要再添加softmax。（以下文章有提及：<a href="https://blog.csdn.net/zhufenghao/article/details/52735750" target="_blank" rel="noopener">1</a>，<a href="https://blog.csdn.net/haiyuanboy/article/details/90377817" target="_blank" rel="noopener">2</a>）</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/95386061" target="_blank" rel="noopener">交叉熵损失</a>：扩大低概率高损失、高概率低损失的差距，同样使得损失函数对网络输出“更敏感”，更有利于分类。</p>
<p>——<a href="https://zhuanlan.zhihu.com/p/37217242" target="_blank" rel="noopener">常见损失函数小结</a></p>
</blockquote>
<h4 id="交叉熵与均方误差损失函数对比"><a href="#交叉熵与均方误差损失函数对比" class="headerlink" title="交叉熵与均方误差损失函数对比"></a>交叉熵与均方误差损失函数对比</h4><p>交叉熵只看重正确分类的结果，而均方差对每个输出结果都看重。</p>
<blockquote>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf3bno6qd9j31hs0ju794.jpg" alt="截屏2020-05-24 上午3.17.13"></p>
<p>——<a href="https://blog.csdn.net/blogshinelee/article/details/103518097" target="_blank" rel="noopener">直观理解为什么分类问题用交叉熵损失而不用均方误差损失?</a></p>
<p><strong>交叉熵的损失函数只和分类正确的预测结果有关系，而MSE的损失函数还和错误的分类有关系，该分类函数除了让正确的分类尽量变大，还会让错误的分类变得平均，但实际在分类问题中这个调整是没有必要的。但是对于回归问题来说，这样的考虑就显得很重要了。所以，回归问题熵使用交叉上并不合适。</strong><br>——<a href="https://blog.csdn.net/weixin_41888969/article/details/89450163" target="_blank" rel="noopener">为什么均方差（MSE）不适合分类问题？交叉熵（cross-entropy）不适合回归问题？</a></p>
<ol>
<li>神经网络中如果预测值与实际值的误差越大，那么在反向传播训练的过程中，各种参数调整的幅度就要更大，从而使训练更快收敛，如果预测值与实际值的误差小，各种参数调整的幅度就要小，从而减少震荡。</li>
<li><strong>使用均方误差损失函数，误差增大参数的梯度会增大（梯度大，告诉权重要向这个方向更新），但是当误差很大时，参数的梯度就会又减小了（权重又不往这个方向更新了，这不合理）。</strong></li>
<li>使用交叉熵损失函数，误差越大参数的梯度也越大，能够快速收敛。</li>
</ol>
<p>——<a href="https://blog.csdn.net/liuweiyuxiang/article/details/90707375" target="_blank" rel="noopener">分类问题为什么要使用交叉熵损失函数而不是均方误差</a></p>
</blockquote>
<h3 id="激活函数-损失函数"><a href="#激活函数-损失函数" class="headerlink" title="激活函数+损失函数"></a>激活函数+损失函数</h3><blockquote>
<p>MSE均方误差+Sigmoid激活函数使得神经网络反向传播的起始位置——输出层神经元学习率缓慢。</p>
<p>交叉熵损失+Sigmoid激活函数可以解决输出层神经元学习率缓慢的问题，但是不能解决隐藏层神经元学习率缓慢的问题。</p>
<ul>
<li><p>改变激活函数或损失函数有可能解决该问题：</p>
<p>激活函数：将sigmoid函数，改为不会造成梯度消失的函数，例如：ReLU函数，不仅能解决输出层学习率缓慢，还能解决隐藏层学习率缓慢问题。</p>
<p>损失函数：将均方误差损失，改为交叉熵损失（<a href="https://blog.csdn.net/blogshinelee/article/details/103518097" target="_blank" rel="noopener">直观理解为什么分类问题用交叉熵损失而不用均方误差损失?</a>）</p>
</li>
</ul>
<p>——<a href="https://zhuanlan.zhihu.com/p/37217242" target="_blank" rel="noopener">常见损失函数小结</a></p>
<p>只要激活函数是sigmoid，tanh类似的曲线，不适合使用MSE，需要搭配交叉熵损失函数。</p>
<p>对于MSE，随着误差的增大，权值需要调整的幅度先变大后变小，这就导致当误差很大时，模型显得“自暴自弃”不肯学习</p>
<p>——<a href="https://zhuanlan.zhihu.com/p/63731947" target="_blank" rel="noopener">为什么使用交叉熵作为损失函数</a>，评论</p>
</blockquote>
<h3 id="优化器-Optimizer"><a href="#优化器-Optimizer" class="headerlink" title="优化器 Optimizer"></a>优化器 Optimizer</h3><h4 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h4><blockquote>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf43xiyod1g30h80dc4n1.gif" alt="img" style="zoom: 50%;" /></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf43xmkc6bg30h80dc1ca.gif" alt="img" style="zoom:50%;" /></p>
<p>——<a href="https://cs231n.github.io/neural-networks-3/" target="_blank" rel="noopener">Convolutional Neural Networks for Visual Recognition</a></p>
</blockquote>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/05/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E4%BB%8B%E7%BB%8DVGG%E7%BD%91%E7%BB%9C%E7%9A%84%E6%96%87%E7%AB%A0%E9%93%BE%E6%8E%A5/" rel="prev" title="关于VGG网络的文章链接">
      <i class="fa fa-chevron-left"></i> 关于VGG网络的文章链接
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/05/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/" rel="next" title="梯度消失和梯度爆炸问题解决方案">
      梯度消失和梯度爆炸问题解决方案 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络中的常见问题"><span class="nav-text">神经网络中的常见问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度爆炸"><span class="nav-text">梯度爆炸</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#激活函数"><span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#sigmoid"><span class="nav-text">sigmoid</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tanh"><span class="nav-text">tanh</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ReLU"><span class="nav-text">ReLU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Leaky-ReLU-PReLU（Parametric-Relu）-RReLU（Random-ReLU）"><span class="nav-text">Leaky ReLU, PReLU（Parametric Relu）, RReLU（Random ReLU）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数"><span class="nav-text">损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#交叉熵损失"><span class="nav-text">交叉熵损失</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#交叉熵与均方误差损失函数对比"><span class="nav-text">交叉熵与均方误差损失函数对比</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#激活函数-损失函数"><span class="nav-text">激活函数+损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#优化器-Optimizer"><span class="nav-text">优化器 Optimizer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#比较"><span class="nav-text">比较</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="liaoooyx"
      src="/images/header.jpg">
  <p class="site-author-name" itemprop="name">liaoooyx</p>
  <div class="site-description" itemprop="description">坐标英国利兹</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">83</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">145</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="motion-element">
    <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=280 height=52 src="//music.163.com/outchain/player?type=2&id=34383149&auto=0&height=32"></iframe>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/liaoooyx" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;liaoooyx" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/u011848397" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;u011848397" rel="noopener" target="_blank"><i class="book fa-fw"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="rss fa-fw"></i>RSS</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:727687719@gmail.com" title="E-Mail → mailto:727687719@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">liaoooyx</span>
  
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>









<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>





  <script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>



  <script async src="/js/cursor/text.js"></script>





  <script src="/js/wobblewindow.js"></script>
  <script>
    //只在桌面版网页启用特效
    if( window.innerWidth > 768  ){
      $(document).ready(function () {
        
          $('.header').wobbleWindow({
          	movementTop: false,
            movementLeft: false,
            movementRight: false,
          	radius: 50,
          	wobbleFactor: 0.93, //震荡持续时间
     		wobbleSpeed: 0.03 //震荡速度
          });
        

       //  
       //    $('.sidebar').wobbleWindow({
       //    	position: 'fixed',
       //      radius: 10,
       //    	wobbleFactor: 0.99, //震荡持续时间
     		// wobbleSpeed: 0.01 //震荡速度
       //    });
       //  

      });
    }
  </script>

  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  


</body>
</html>
